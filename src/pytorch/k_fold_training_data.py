import logging
from random import shuffle as randshuffle
import torch
import numpy as np
import random
from torch.utils.data import DataLoader, random_split

from src.pytorch.training_data import (
    InstanceDataset,
    load_training_state_value_pairs,
)

_log = logging.getLogger(__name__)

class KFoldTrainingData:
    def __init__(self, samples_file, batch_size=100, num_folds=10, output_layer="regression", shuffle=False, seed=-1):
        # Getting training data from sas_plan generated by the sampling engine.
        self.state_value_pairs, self.domain_max_value = load_training_state_value_pairs(
            samples_file
        )
        self.batch_size = batch_size
        self.num_folds = num_folds
        self.output_layer = output_layer
        self.shuffle = shuffle
        self.seed = seed
        self.kfolds = self.generate_kfold_training_data()

    def generate_kfold_training_data(self):
        """
        Generate the folds.
        Return two list of tuples of size num_folds: dataloaders and problems.
        The first item corresponds to train set, and the second to test set.
        """

        _log.info(f"Generating {self.num_folds}-fold...")

        kfolds = []
        instances_per_fold = int(len(self.state_value_pairs) / self.num_folds)
        for i in range(self.num_folds):
            training_set, test_set = [], []

            for j in range(len(self.state_value_pairs)):
                if self.num_folds == 1:
                    train_split = 0.8
                    train_size = int(train_split * len(self.state_value_pairs))
                    val_size = len(self.state_value_pairs) - train_size
                    training_set, test_set = random_split(self.state_value_pairs, [train_size, val_size])
                else:
                    if int(j / instances_per_fold) == i:
                        test_set.append(self.state_value_pairs[j])
                    else:
                        training_set.append(self.state_value_pairs[j])

            worker_fn = None if self.seed == -1 else seed_worker
            g = None if self.seed == -1 else torch.Generator()
            if g != None:
                g.manual_seed(self.seed)

            train_dataloader = DataLoader(
                dataset=InstanceDataset(training_set, self.domain_max_value, self.output_layer),
                batch_size=self.batch_size,
                shuffle=self.shuffle,
                num_workers=1,
                worker_init_fn=seed_worker,
                generator=g,
            )
            test_dataloader = DataLoader(
                dataset=InstanceDataset(test_set, self.domain_max_value, self.output_layer),
                batch_size=self.batch_size,
                shuffle=self.shuffle,
                num_workers=1,
                worker_init_fn=seed_worker,
                generator=g,
            )
            kfolds.append((train_dataloader, test_dataloader))

        return kfolds

    def get_fold(self, idx):
        """
        Returns a fold as tuple(train dataloader, test dataloader).
        Counting from 0.
        """
        return self.kfolds[idx]


def seed_worker(worker_id):
    """
    Sets the seed of each worker.
    See: https://pytorch.org/docs/stable/notes/randomness.html
    """
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)
